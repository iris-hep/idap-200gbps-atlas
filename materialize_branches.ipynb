{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward: 2.6.3\n",
      "dask-awkward: 2024.3.0\n",
      "uproot: 5.3.2\n",
      "hist: 2.7.2\n",
      "coffea: 2024.4.1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "from dask.distributed import Client\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "from coffea import dataset_tools\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from input_files import utils\n",
    "\n",
    "from dask.distributed import LocalCluster, Client, progress, performance_report\n",
    "\n",
    "# local: single thread, single worker\n",
    "# cluster = LocalCluster(n_workers=1, processes=False, threads_per_worker=1)\n",
    "# client = Client(cluster)\n",
    "\n",
    "# for UChicago\n",
    "# update this to point to your own client!\n",
    "client = Client(\"tcp://dask-alheld-3e6e9ab9-0.af-jupyter:8786\")\n",
    "\n",
    "print(f\"awkward: {ak.__version__}\")\n",
    "print(f\"dask-awkward: {dak.__version__}\")\n",
    "print(f\"uproot: {uproot.__version__}\")\n",
    "print(f\"hist: {hist.__version__}\")\n",
    "print(f\"coffea: {coffea.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef7dd8d-68e5-43fa-b8bf-5d8f972520c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileset summary\n",
      " - number of files: 30\n",
      "cannot determine total size / number of events when max_files_per_container is being used\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# INPUT CONFIGURATION\n",
    "# -------------------\n",
    "# modify this to change how many files are being processed\n",
    "# top-level processes determine containers/DSIDs, which each have some number of files\n",
    "# full list is list(find_containers.container_dict.keys()) + [\"data15_13TeV\", \"data16_13TeV\", \"data17_13TeV\", \"data18_13TeV\"]\n",
    "\n",
    "PROCESSES_TO_USE = [\"ttbar\"]  # 6.7 TB\n",
    "# PROCESSES_TO_USE = [\"db\", \"zjets\", \"wjets\", \"ttV\", \"othertop\", \"ttbar\"]  # all simulation, 48.4 TB\n",
    "# PROCESSES_TO_USE = [\"db\", \"zjets\", \"wjets\", \"ttV\", \"othertop\", \"ttbar\", \"data15_13TeV\", \"data16_13TeV\", \"data17_13TeV\", \"data18_13TeV\"]  # 191 TB\n",
    "\n",
    "fileset = utils.get_fileset(PROCESSES_TO_USE, max_files_per_container=10, max_containers_per_dsid=None, max_dsid_per_process=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff312282-a87b-4394-aff4-6acc12d27c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files at MWT2: 30, elsewhere: 0\n"
     ]
    }
   ],
   "source": [
    "# check for files not yet replicated to MWT2\n",
    "files_at_mwt2 = 0\n",
    "files_elsewhere = 0\n",
    "for process in fileset.keys():\n",
    "    for file in fileset[process][\"files\"]:\n",
    "        if \"mwt2\" in file:\n",
    "            files_at_mwt2 += 1\n",
    "        else:\n",
    "            files_elsewhere += 1\n",
    "\n",
    "print(f\"files at MWT2: {files_at_mwt2}, elsewhere: {files_elsewhere}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd0ec454-7620-4c63-930a-2c28ffa6efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn fileset into simple list of files to run over\n",
    "all_files = []\n",
    "for process in fileset:\n",
    "    all_files += fileset[process][\"files\"]\n",
    "\n",
    "# define work to be done\n",
    "def uproot_open_materialize(fname):\n",
    "    BRANCH_LIST = [\n",
    "        \"AnalysisJetsAuxDyn.pt\", \"AnalysisJetsAuxDyn.eta\", \"AnalysisJetsAuxDyn.phi\", \"AnalysisJetsAuxDyn.m\",\n",
    "        \"AnalysisElectronsAuxDyn.pt\", \"AnalysisElectronsAuxDyn.eta\", \"AnalysisElectronsAuxDyn.phi\",\n",
    "        \"AnalysisElectronsAuxDyn.m\", \"AnalysisMuonsAuxDyn.pt\", \"AnalysisMuonsAuxDyn.eta\",\n",
    "        \"AnalysisMuonsAuxDyn.phi\", \"AnalysisJetsAuxDyn.EnergyPerSampling\", \"AnalysisJetsAuxDyn.SumPtTrkPt500\",\n",
    "        \"AnalysisJetsAuxDyn.TrackWidthPt1000\", \"PrimaryVerticesAuxDyn.z\", \"PrimaryVerticesAuxDyn.x\",\n",
    "        \"PrimaryVerticesAuxDyn.y\", \"AnalysisJetsAuxDyn.NumTrkPt500\", \"AnalysisJetsAuxDyn.NumTrkPt1000\",\n",
    "        \"AnalysisJetsAuxDyn.SumPtChargedPFOPt500\", \"AnalysisJetsAuxDyn.Timing\",\n",
    "        \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_eta\", \"AnalysisJetsAuxDyn.ActiveArea4vec_eta\",\n",
    "        \"AnalysisJetsAuxDyn.DetectorEta\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_phi\",\n",
    "        \"AnalysisJetsAuxDyn.ActiveArea4vec_phi\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_m\",\n",
    "        \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_pt\", \"AnalysisJetsAuxDyn.EMFrac\",\n",
    "        \"AnalysisJetsAuxDyn.Width\", \"AnalysisJetsAuxDyn.ActiveArea4vec_m\", \"AnalysisJetsAuxDyn.ActiveArea4vec_pt\",\n",
    "        \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksWidth\", \"AnalysisJetsAuxDyn.PSFrac\",\n",
    "        \"AnalysisJetsAuxDyn.JVFCorr\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksC1\",\n",
    "        \"AnalysisJetsAuxDyn.DFCommonJets_fJvt\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_NTracks\",\n",
    "        \"AnalysisJetsAuxDyn.GhostMuonSegmentCount\", \"AnalysisMuonsAuxDyn.muonSegmentLinks\",\n",
    "        \"AnalysisMuonsAuxDyn.msOnlyExtrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.extrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.inDetTrackParticleLink\", \"AnalysisMuonsAuxDyn.muonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.momentumBalanceSignificance\", \"AnalysisMuonsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "        \"AnalysisMuonsAuxDyn.scatteringCurvatureSignificance\", \"AnalysisMuonsAuxDyn.scatteringNeighbourSignificance\",\n",
    "        \"AnalysisMuonsAuxDyn.neflowisol20_CloseByCorr\", \"AnalysisMuonsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisMuonsAuxDyn.topoetcone30\", \"AnalysisMuonsAuxDyn.topoetcone40\", \"AnalysisMuonsAuxDyn.neflowisol20\",\n",
    "        \"AnalysisMuonsAuxDyn.segmentDeltaEta\", \"AnalysisMuonsAuxDyn.DFCommonJetDr\",\n",
    "        \"AnalysisMuonsAuxDyn.combinedTrackParticleLink\", \"AnalysisMuonsAuxDyn.InnerDetectorPt\",\n",
    "        \"AnalysisMuonsAuxDyn.MuonSpectrometerPt\", \"AnalysisMuonsAuxDyn.clusterLink\",\n",
    "        \"AnalysisMuonsAuxDyn.spectrometerFieldIntegral\", \"AnalysisElectronsAuxDyn.ambiguityLink\",\n",
    "        \"AnalysisMuonsAuxDyn.EnergyLoss\", \"AnalysisJetsAuxDyn.NNJvtPass\", \"AnalysisElectronsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "        \"AnalysisElectronsAuxDyn.topoetcone20ptCorrection\", \"AnalysisElectronsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500_CloseByCorr\",\n",
    "        \"AnalysisElectronsAuxDyn.DFCommonElectronsECIDSResult\", \"AnalysisElectronsAuxDyn.neflowisol20\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500\", \"AnalysisMuonsAuxDyn.ptcone40\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000_CloseByCorr\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000\", \"AnalysisMuonsAuxDyn.ptvarcone40\",\n",
    "        \"AnalysisElectronsAuxDyn.f1\", \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt500\",\n",
    "        \"PrimaryVerticesAuxDyn.vertexType\", \"AnalysisMuonsAuxDyn.ptvarcone30\", \"AnalysisMuonsAuxDyn.ptcone30\",\n",
    "        \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt1000\",\n",
    "        \"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt500\", \"AnalysisMuonsAuxDyn.CaloLRLikelihood\"\n",
    "    ]\n",
    "    \n",
    "    filter_name = lambda x: x in BRANCH_LIST\n",
    "\n",
    "    size_uncompressed = 0\n",
    "    failed = []\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        with uproot.open(fname, filter_name=filter_name) as f:\n",
    "            num_entries = f[\"CollectionTree\"].num_entries\n",
    "            for b in BRANCH_LIST:\n",
    "                print(b)\n",
    "                f[\"CollectionTree\"][b].array()\n",
    "                size_uncompressed += f[\"CollectionTree\"][b].uncompressed_bytes\n",
    "    \n",
    "            size_read = f.file.source.num_requested_bytes\n",
    "        \n",
    "    except OSError:\n",
    "        num_entries = 0\n",
    "        size_read = 0\n",
    "        size_uncompressed = 0\n",
    "        failed.append(fname)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    return {\"read\": size_read, \"uncompressed\": size_uncompressed, \"num_entries\": num_entries, \"runtime\": t1-t0, \"failed\": failed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc64663-8ca0-46f8-8ca1-54c1aac51979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with 30 files\n",
      "wall clock time: 37.31s\n",
      "current number of workers: 1\n"
     ]
    }
   ],
   "source": [
    "# perform computation\n",
    "print(f\"running with {len(all_files)} files\")\n",
    "scattered_data = client.scatter([f for f in all_files])  # instead of submitting (possibly big) object directly\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "with performance_report(filename=\"dask-report-plain-uproot.html\"):\n",
    "    futures = client.map(uproot_open_materialize, scattered_data)\n",
    "    out = ak.Array([r for r in client.gather(iter(futures))])\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"wall clock time: {t1-t0:.2f}s\")\n",
    "print(f\"current number of workers: {len(client.scheduler_info()['workers'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e786f1a-2df7-4e35-95f9-2438512a17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total read (compressed): 1.53 GB\n",
      "total read (uncompressed): 5.27 GB\n",
      "average data rate: 0.33 Gbps (need to scale by x611 to reach 200 Gbps)\n",
      "total event rate (wall clock time): 19.03 kHz (processed 710000 events total)\n",
      "total aggregated runtime in function: 383.93 s\n",
      "ratio total runtime / wall clock time: 10.29 (should match # cores without overhead / scheduling issues)\n",
      "event rate (aggregated time spent in function): 1.85 kHz\n",
      "0 file(s) failed:\n"
     ]
    }
   ],
   "source": [
    "# summary of performance\n",
    "read_GB = sum(out['read']) / 1000**3\n",
    "print(f\"total read (compressed): {read_GB:.2f} GB\")\n",
    "print(f\"total read (uncompressed): {sum(out['uncompressed']) / 1000**3:.2f} GB\")\n",
    "\n",
    "rate_Gbps = read_GB*8/(t1-t0)\n",
    "print(f\"average data rate: {rate_Gbps:.2f} Gbps (need to scale by x{200/rate_Gbps:.0f} to reach 200 Gbps)\")\n",
    "\n",
    "n_evts = sum(out[\"num_entries\"])\n",
    "print(f\"total event rate (wall clock time): {n_evts / (t1-t0) / 1000:.2f} kHz (processed {n_evts} events total)\")\n",
    "\n",
    "total_runtime = sum(out[\"runtime\"])\n",
    "print(f\"total aggregated runtime in function: {total_runtime:.2f} s\")\n",
    "print(f\"ratio total runtime / wall clock time: {total_runtime / (t1-t0):.2f} \"\\\n",
    "      \"(should match # cores without overhead / scheduling issues)\")\n",
    "print(f\"event rate (aggregated time spent in function): {n_evts / total_runtime / 1000:.2f} kHz\")\n",
    "\n",
    "failed = ak.flatten(out[\"failed\"])\n",
    "print(f\"{len(failed)} file(s) failed:\")\n",
    "for fname in failed:\n",
    "    print(f\"  {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcc374-834f-4aa9-8975-0118a98ddd35",
   "metadata": {},
   "source": [
    "# everything below: coffea 2024 approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38efe4-8024-422c-ba42-12bd3a3b44cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BRANCH_LIST = [\n",
    "    \"AnalysisJetsAuxDyn.pt\", \"AnalysisJetsAuxDyn.eta\", \"AnalysisJetsAuxDyn.phi\", \"AnalysisJetsAuxDyn.m\",\n",
    "    \"AnalysisElectronsAuxDyn.pt\", \"AnalysisElectronsAuxDyn.eta\", \"AnalysisElectronsAuxDyn.phi\",\n",
    "    \"AnalysisElectronsAuxDyn.m\", \"AnalysisMuonsAuxDyn.pt\", \"AnalysisMuonsAuxDyn.eta\",\n",
    "    \"AnalysisMuonsAuxDyn.phi\", \"AnalysisJetsAuxDyn.EnergyPerSampling\", \"AnalysisJetsAuxDyn.SumPtTrkPt500\",\n",
    "    \"AnalysisJetsAuxDyn.TrackWidthPt1000\", \"PrimaryVerticesAuxDyn.z\", \"PrimaryVerticesAuxDyn.x\",\n",
    "    \"PrimaryVerticesAuxDyn.y\", \"AnalysisJetsAuxDyn.NumTrkPt500\", \"AnalysisJetsAuxDyn.NumTrkPt1000\",\n",
    "    \"AnalysisJetsAuxDyn.SumPtChargedPFOPt500\", \"AnalysisJetsAuxDyn.Timing\",\n",
    "    \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_eta\", \"AnalysisJetsAuxDyn.ActiveArea4vec_eta\",\n",
    "    \"AnalysisJetsAuxDyn.DetectorEta\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_phi\",\n",
    "    \"AnalysisJetsAuxDyn.ActiveArea4vec_phi\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_m\",\n",
    "    \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_pt\", \"AnalysisJetsAuxDyn.EMFrac\",\n",
    "    \"AnalysisJetsAuxDyn.Width\", \"AnalysisJetsAuxDyn.ActiveArea4vec_m\", \"AnalysisJetsAuxDyn.ActiveArea4vec_pt\",\n",
    "    \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksWidth\", \"AnalysisJetsAuxDyn.PSFrac\",\n",
    "    \"AnalysisJetsAuxDyn.JVFCorr\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksC1\",\n",
    "    \"AnalysisJetsAuxDyn.DFCommonJets_fJvt\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_NTracks\",\n",
    "    \"AnalysisJetsAuxDyn.GhostMuonSegmentCount\", \"AnalysisMuonsAuxDyn.muonSegmentLinks\",\n",
    "    \"AnalysisMuonsAuxDyn.msOnlyExtrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "    \"AnalysisMuonsAuxDyn.extrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "    \"AnalysisMuonsAuxDyn.inDetTrackParticleLink\", \"AnalysisMuonsAuxDyn.muonSpectrometerTrackParticleLink\",\n",
    "    \"AnalysisMuonsAuxDyn.momentumBalanceSignificance\", \"AnalysisMuonsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "    \"AnalysisMuonsAuxDyn.scatteringCurvatureSignificance\", \"AnalysisMuonsAuxDyn.scatteringNeighbourSignificance\",\n",
    "    \"AnalysisMuonsAuxDyn.neflowisol20_CloseByCorr\", \"AnalysisMuonsAuxDyn.topoetcone20\",\n",
    "    \"AnalysisMuonsAuxDyn.topoetcone30\", \"AnalysisMuonsAuxDyn.topoetcone40\", \"AnalysisMuonsAuxDyn.neflowisol20\",\n",
    "    \"AnalysisMuonsAuxDyn.segmentDeltaEta\", \"AnalysisMuonsAuxDyn.DFCommonJetDr\",\n",
    "    \"AnalysisMuonsAuxDyn.combinedTrackParticleLink\", \"AnalysisMuonsAuxDyn.InnerDetectorPt\",\n",
    "    \"AnalysisMuonsAuxDyn.MuonSpectrometerPt\", \"AnalysisMuonsAuxDyn.clusterLink\",\n",
    "    \"AnalysisMuonsAuxDyn.spectrometerFieldIntegral\", \"AnalysisElectronsAuxDyn.ambiguityLink\",\n",
    "    \"AnalysisMuonsAuxDyn.EnergyLoss\", \"AnalysisJetsAuxDyn.NNJvtPass\", \"AnalysisElectronsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "    \"AnalysisElectronsAuxDyn.topoetcone20ptCorrection\", \"AnalysisElectronsAuxDyn.topoetcone20\",\n",
    "    \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500_CloseByCorr\",\n",
    "    \"AnalysisElectronsAuxDyn.DFCommonElectronsECIDSResult\", \"AnalysisElectronsAuxDyn.neflowisol20\",\n",
    "    \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500\", \"AnalysisMuonsAuxDyn.ptcone40\",\n",
    "    \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000_CloseByCorr\",\n",
    "    \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000\", \"AnalysisMuonsAuxDyn.ptvarcone40\",\n",
    "    \"AnalysisElectronsAuxDyn.f1\", \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt500\",\n",
    "    \"PrimaryVerticesAuxDyn.vertexType\", \"AnalysisMuonsAuxDyn.ptvarcone30\", \"AnalysisMuonsAuxDyn.ptcone30\",\n",
    "    \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt1000\",\n",
    "    \"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt500\", \"AnalysisMuonsAuxDyn.CaloLRLikelihood\"\n",
    "]\n",
    "\n",
    "def materialize_branches(events):\n",
    "    num_events = ak.num(events, axis=0)  # track number of events\n",
    "\n",
    "    # this will read around 25% of data files\n",
    "    # materialize branches, just derive integers from them that will be aggregated to avoid memory issues\n",
    "    _counter = 0\n",
    "    for branch in BRANCH_LIST:\n",
    "        obj_name, obj_prop = branch.split(\".\")\n",
    "        obj_name = obj_name.replace(\"Analysis\", \"\").replace(\"AuxDyn\", \"\")\n",
    "        if \"Link\" not in obj_prop:\n",
    "            branch_data = events[obj_name, obj_prop]\n",
    "        else:\n",
    "            branch_data = events[obj_name, obj_prop][\"m_persIndex\"]\n",
    "\n",
    "        _counter_to_add = ak.count_nonzero(branch_data, axis=-1)  # reduce innermost\n",
    "\n",
    "        # reduce >2-dimensional (per event) branches further\n",
    "        for _ in range(_counter_to_add.ndim - 1):\n",
    "            _counter_to_add = ak.count_nonzero(_counter_to_add, axis=-1)\n",
    "\n",
    "        _counter = _counter + _counter_to_add  # sum 1-dim array built from new branch\n",
    "\n",
    "    _counter = ak.count_nonzero(_counter, axis=0)  # reduce to int\n",
    "\n",
    "    return {\"nevts\": num_events, \"_counter\": _counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc41-0a72-44ec-a8b8-654286a02196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-process\n",
    "with performance_report(filename=\"dask-report-preprocess.html\"):\n",
    "    samples, report = dataset_tools.preprocess(fileset, skip_bad_files=True, uproot_options={\"allow_read_errors_with_report\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf89ae-569f-4191-963d-d0e122eaf7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find issues where access did not work\n",
    "for process in report:\n",
    "    for k, v in report[process][\"files\"].items():\n",
    "        if v[\"steps\"] is None:\n",
    "            print(f\"could not read {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15835a57-1182-4efb-8306-07f36af7a5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the task graph\n",
    "# filter_name seems to not do anything here in terms of performance\n",
    "filter_name = lambda name: name in BRANCH_LIST\n",
    "tasks = dataset_tools.apply_to_fileset(materialize_branches,\n",
    "                                       samples,\n",
    "                                       uproot_options={\"allow_read_errors_with_report\": (OSError, TypeError, KeyError), \"filter_name\": filter_name},\n",
    "                                       schemaclass=PHYSLITESchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dfb17-6806-46c8-aa59-cd475e40cf64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute task graph\n",
    "t0 = time.perf_counter()\n",
    "with performance_report(filename=\"dask-report-compute.html\"):\n",
    "    ((out, report),) = dask.compute(tasks)  # feels strange that this is a tuple-of-tuple\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"total time spent in uproot reading data: {ak.sum([v['duration'] for v in report.values()]):.2f} s\")\n",
    "print(f\"wall time: {t1-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47e9e5-57ea-4385-b8f8-b2a22ded030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"output: {out}\")\n",
    "\n",
    "print(\"\\nperformance metrics:\")\n",
    "event_rate = sum([out[process][\"nevts\"] for process in out.keys()]) / (t1-t0)\n",
    "print(f\" - event rate: {event_rate / 1_000:.2f} kHz\")\n",
    "\n",
    "# need uproot>=5.3.2 to get these useful performance stats\n",
    "num_bytes = ak.sum([report[process][\"performance_counters\"][\"num_requested_bytes\"] for process in out.keys()])\n",
    "read_MB = num_bytes / 1_000**2\n",
    "rate_Mbs = read_MB / (t1-t0)\n",
    "print(f\" - read {read_MB:.2f} MB in {t1-t0:.2f} s -> {rate_Mbs*8:.2f} Mbps (need to scale by x{200/8/rate_Mbs*1000:.0f} to reach 200 Gbps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da90743-8045-42f3-a59c-12c7ff9f8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report problematic files that caused exceptions\n",
    "for process in report.keys():\n",
    "    for i_file in range(len(report[process].exception)):\n",
    "        file_report = report[process][i_file]\n",
    "        if file_report.exception is not None:\n",
    "            print(file_report.args[0].strip(\"\\'\"))\n",
    "            print(file_report.message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb1ab6-0be4-4b99-942a-c1b53eda5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that the right colums are being touched\n",
    "# dak.report_necessary_columns(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25225440-d57c-49e7-b48e-43fe5362327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if issues with files exist, paste in path and reproduce\n",
    "# fname = \"root://192.170.240.143//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/mc20_13TeV/6e/cf/DAOD_PHYSLITE.37230006._000001.pool.root.1\"\n",
    "# treename = \"CollectionTree\"\n",
    "# events = NanoEventsFactory.from_root({fname: treename}, schemaclass=PHYSLITESchema).events()\n",
    "# task = materialize_branches(events)\n",
    "# task[\"_counter\"].compute()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version,-language_info.version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
