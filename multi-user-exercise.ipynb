{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd755c3b-05f4-4598-bf5d-90c83da047e6",
   "metadata": {},
   "source": [
    "# Multi-user exercise at 2024 IRIS-HEP retreat\n",
    "\n",
    "***Important:*** add a new cell below with your Dask cluster (see instructions).\n",
    "It should look similar to the following:\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://dask-alheld-4027af07-8.af-jupyter:8786\")\n",
    "client\n",
    "```\n",
    "\n",
    "Your client address will be different (the one in the example above won't work for you!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517e1d5-c051-46ee-b258-cae3a69a476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your cell here to connect to your own client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2503d-bcf7-46fb-a8a2-5f6e55c8b7f9",
   "metadata": {},
   "source": [
    "We will stagger the data processing a bit by waiting up to 5 minutes per user: this is meant to capture the reality of multi-user environments a bit better, where not everyone launches at the exact same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e85672-0331-4303-8083-a9fccb8aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "time_to_wait = random.uniform(0, 300)\n",
    "print(f\"waiting for {time_to_wait:.0f} seconds before running the rest of the notebook\")\n",
    "\n",
    "for i in range(10):\n",
    "    time.sleep(time_to_wait/10)\n",
    "    print(f\"{(i+1)/10:.0%} of waiting time done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f4d32-e5c4-42f9-84e8-430034493a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import time\n",
    "import warnings\n",
    "import copy\n",
    "import pathlib\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import hist.dask\n",
    "import coffea\n",
    "import numpy as np\n",
    "import uproot\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.style.use(\"ggplot\")\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "from coffea import dataset_tools\n",
    "\n",
    "import utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import input_files.utils\n",
    "\n",
    "from dask.distributed import LocalCluster, Client, progress, performance_report\n",
    "\n",
    "# create a folder for output tracking of uproot.open setup\n",
    "MEASUREMENT_PATH = pathlib.Path(datetime.datetime.now().strftime(\"measurements/%Y-%m-%d_%H-%M-%S\"))\n",
    "os.makedirs(MEASUREMENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7dd8d-68e5-43fa-b8bf-5d8f972520c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# INPUT CONFIGURATION\n",
    "# -------------------\n",
    "\n",
    "PROCESSES_TO_USE = [\"ttbar\", \"othertop\"]  # 9 TB\n",
    "# PROCESSES_TO_USE = [\"db\", \"zjets\", \"wjets\", \"ttV\", \"othertop\", \"ttbar\", \"data15_13TeV\", \"data16_13TeV\", \"data17_13TeV\", \"data18_13TeV\"]  # 190 TB\n",
    "\n",
    "fileset = input_files.utils.get_fileset(PROCESSES_TO_USE, max_files_per_container=None, max_containers_per_dsid=None, max_dsid_per_process=None)\n",
    "\n",
    "utils.save_fileset(fileset, MEASUREMENT_PATH)\n",
    "print(f\"total number of files (including duplicates): {sum([len(v['files']) for v in fileset.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c438a9f-372a-4968-9758-466b52ad30e3",
   "metadata": {},
   "source": [
    "## Dask distributing `uproot.open`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ec454-7620-4c63-930a-2c28ffa6efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn fileset into simple list of files to run over\n",
    "all_files = []\n",
    "for process in fileset:\n",
    "    all_files += fileset[process][\"files\"]\n",
    "\n",
    "# define work to be done\n",
    "def uproot_open_materialize(fname):\n",
    "    # ~15%, around 300 Mbps single core, ~130 Mbps with 100 workers\n",
    "    BRANCH_LIST = [\n",
    "        \"InDetTrackParticlesAuxDyn.definingParametersCovMatrixOffDiag\",\n",
    "        'PrimaryVerticesAuxDyn.z',\n",
    "        'PrimaryVerticesAuxDyn.x',\n",
    "        'PrimaryVerticesAuxDyn.y',\n",
    "        'AnalysisJetsAuxDyn.Timing',\n",
    "        'AnalysisJetsAuxDyn.JetConstitScaleMomentum_phi',\n",
    "        'AnalysisJetsAuxDyn.DetectorEta',\n",
    "        'AnalysisJetsAuxDyn.ActiveArea4vec_eta',\n",
    "        'AnalysisJetsAuxDyn.JetConstitScaleMomentum_eta',\n",
    "        'AnalysisJetsAuxDyn.phi',\n",
    "        'AnalysisJetsAuxDyn.m',\n",
    "        'AnalysisJetsAuxDyn.JetConstitScaleMomentum_pt',\n",
    "        'AnalysisJetsAuxDyn.ActiveArea4vec_phi',\n",
    "        'AnalysisJetsAuxDyn.JetConstitScaleMomentum_m',\n",
    "        'AnalysisJetsAuxDyn.ActiveArea4vec_m',\n",
    "        'AnalysisJetsAuxDyn.pt',\n",
    "        'AnalysisJetsAuxDyn.Width',\n",
    "        'AnalysisJetsAuxDyn.EMFrac',\n",
    "        'AnalysisJetsAuxDyn.ActiveArea4vec_pt',\n",
    "        'AnalysisJetsAuxDyn.PSFrac',\n",
    "        'AnalysisJetsAuxDyn.JVFCorr',\n",
    "        'AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksC1',\n",
    "        'AnalysisJetsAuxDyn.eta',\n",
    "        'AnalysisPhotonsAuxDyn.topoetcone40_CloseByCorr',\n",
    "        'AnalysisPhotonsAuxDyn.topoetcone40',\n",
    "        'AnalysisPhotonsAuxDyn.eta',\n",
    "        'AnalysisJetsAuxDyn.DFCommonJets_fJvt',\n",
    "        'AnalysisPhotonsAuxDyn.phi',\n",
    "        'AnalysisPhotonsAuxDyn.topoetcone20_CloseByCorr',\n",
    "        'AnalysisPhotonsAuxDyn.topoetcone40ptCorrection',\n",
    "        'AnalysisPhotonsAuxDyn.topoetcone20ptCorrection',\n",
    "        'AnalysisPhotonsAuxDyn.pt',\n",
    "        'AnalysisJetsAuxDyn.DFCommonJets_QGTagger_NTracks',\n",
    "        'AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksWidth',\n",
    "        'AnalysisJetsAuxDyn.GhostMuonSegmentCount',\n",
    "        'AnalysisPhotonsAuxDyn.topoetcone20',\n",
    "        'AnalysisPhotonsAuxDyn.f1',\n",
    "        'AnalysisPhotonsAuxDyn.DFCommonPhotonsIsEMTightIsEMValue',\n",
    "        'AnalysisPhotonsAuxDyn.ptcone20_CloseByCorr',\n",
    "        'AnalysisPhotonsAuxDyn.OQ',\n",
    "        'AnalysisPhotonsAuxDyn.ptcone20',\n",
    "        'AnalysisTauJetsAuxDyn.RNNJetScore',\n",
    "        'AnalysisTauJetsAuxDyn.JetDeepSetScore',\n",
    "        'AnalysisTauJetsAuxDyn.etaTauEnergyScale',\n",
    "        'AnalysisTauJetsAuxDyn.etaFinalCalib',\n",
    "        'AnalysisTauJetsAuxDyn.RNNEleScoreSigTrans_v1'\n",
    "    ]\n",
    "\n",
    "    filter_name = lambda x: x in BRANCH_LIST\n",
    "\n",
    "    size_uncompressed = 0\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        with uproot.open(fname, filter_name=filter_name) as f:\n",
    "            num_entries = f[\"CollectionTree\"].num_entries\n",
    "\n",
    "            # iterate approach\n",
    "            # for _ in f[\"CollectionTree\"].iterate(expressions=BRANCH_LIST):\n",
    "            #     pass\n",
    "\n",
    "            # branch loop approach\n",
    "            for b in BRANCH_LIST:\n",
    "                f[\"CollectionTree\"][b].array()\n",
    "                size_uncompressed += f[\"CollectionTree\"][b].uncompressed_bytes\n",
    "\n",
    "            size_read = f.file.source.num_requested_bytes\n",
    "        exception = None\n",
    "\n",
    "    except:\n",
    "        num_entries = 0\n",
    "        size_read = 0\n",
    "        size_uncompressed = 0\n",
    "        exception = traceback.format_exc()\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    time_finished = datetime.datetime.now()\n",
    "    return {\"fname\": fname, \"read\": size_read, \"uncompressed\": size_uncompressed, \"num_entries\": num_entries,\n",
    "            \"runtime\": t1-t0, \"time_finished\": time_finished, \"exception\": exception}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a6ac5-db3a-499f-bc3d-23cab5c362a2",
   "metadata": {},
   "source": [
    "The following cell launches the computation.\n",
    "Make sure the cells afterwards also finish: these will write out all the data that is being gathered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc64663-8ca0-46f8-8ca1-54c1aac51979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform computation\n",
    "print(f\"running with {len(all_files)} files\")\n",
    "\n",
    "utils.start_tracking_workers(client, MEASUREMENT_PATH)  # track worker count in background\n",
    "with performance_report(filename=MEASUREMENT_PATH/\"dask-report-plain-uproot.html\"):\n",
    "    tasks = [dask.delayed(uproot_open_materialize)(f) for f in all_files]  # create tasks\n",
    "    t0 = time.perf_counter()\n",
    "    out = ak.Array(dask.compute(*tasks))  # perform computations\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "utils.stop_tracking_workers()\n",
    "\n",
    "print(f\"wall clock time: {t1-t0:.2f}s\")\n",
    "utils.save_measurement(out, t0, t1, MEASUREMENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea00ca-43f9-4e47-bc2c-2ffbad32fe95",
   "metadata": {},
   "source": [
    "while waiting, check out out the XCache output: https://grafana.mwt2.org/d/EKefjM-Sz/af-network-200gbps-challenge?orgId=1&viewPanel=205&from=now-30m&to=now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e786f1a-2df7-4e35-95f9-2438512a17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load measurements from file again\n",
    "timestamps, nworkers, avg_num_workers = utils.get_timestamps_and_counts(MEASUREMENT_PATH)  # worker count info\n",
    "out, t0, t1 = utils.load_measurement(MEASUREMENT_PATH)\n",
    "\n",
    "# summary of performance\n",
    "read_GB = sum(out['read']) / 1000**3\n",
    "print(f\"total read (compressed): {read_GB:.2f} GB\")\n",
    "print(f\"total read (uncompressed): {sum(out['uncompressed']) / 1000**3:.2f} GB\")\n",
    "\n",
    "rate_Gbps = read_GB*8/(t1-t0)\n",
    "print(f\"average data rate: {rate_Gbps:.2f} Gbps (need to scale by x{200/rate_Gbps:.1f} to reach 200 Gbps)\")\n",
    "\n",
    "n_evts = sum(out[\"num_entries\"])\n",
    "print(f\"total event rate (wall clock time): {n_evts / (t1-t0) / 1000:.2f} kHz (processed {n_evts} events total)\")\n",
    "\n",
    "total_runtime = sum(out[\"runtime\"])\n",
    "print(f\"total aggregated runtime in function: {total_runtime:.2f} s\")\n",
    "print(f\"ratio total runtime / wall clock time: {total_runtime / (t1-t0):.2f} \"\\\n",
    "      \"(should match # cores without overhead / scheduling issues)\")\n",
    "print(f\"time-averaged number of workers: {avg_num_workers:.1f}\")\n",
    "print(f\"\\\"efficiency\\\" (ratio of two numbers above): {total_runtime / (t1-t0) / avg_num_workers:.1%}\")\n",
    "print(f\"event rate (aggregated time spent in function): {n_evts / total_runtime / 1000:.2f} kHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ae2c8-4c83-43b2-9f46-3c8163add495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get arrays for starting time, runtime and end time of all tasks\n",
    "runtimes = np.asarray([datetime.timedelta(seconds=t) for t in out[\"runtime\"]], dtype=np.timedelta64)\n",
    "ends = out[\"time_finished\"].to_numpy()\n",
    "starts = ends - runtimes\n",
    "\n",
    "# calculate instantaneous rates for given timestamp\n",
    "times_for_rates = []\n",
    "instantaneous_rates = []\n",
    "for t in timestamps[::10]:  # only calculate every 30 seconds\n",
    "    mask = np.logical_and((starts <= t), (t <= ends))  # mask for tasks running at given timestamp\n",
    "    rate_Gbps_at_timestamp = sum(out[mask]['read']*8 / 1000**3 / out[mask][\"runtime\"])\n",
    "    times_for_rates.append(t)\n",
    "    instantaneous_rates.append(rate_Gbps_at_timestamp)\n",
    "\n",
    "utils.plot_worker_count(timestamps, nworkers, avg_num_workers, times_for_rates, instantaneous_rates, MEASUREMENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136da759-b7ea-49c2-afab-fe4e12c5422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sum(o is not None for o in out['exception'])} files failed\\n\")\n",
    "\n",
    "# use below to get full list with details\n",
    "# for report in out:\n",
    "#     if report[\"exception\"] is not None:\n",
    "#         print(f\"{report['fname']} failed in {report['runtime']:.2f} s\\n{report['exception']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51de7a-722e-4bf8-8a3e-f39861b0261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime distribution for all files\n",
    "fig, ax = plt.subplots() \n",
    "bins = np.linspace(0, max(out[\"runtime\"])*1.01, 100)\n",
    "ax.hist(out[\"runtime\"], bins=bins)\n",
    "ax.set_xlabel(\"runtime [s]\")\n",
    "ax.set_xlim([0, ax.get_xlim()[1]])\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.semilogy()\n",
    "fig.savefig(MEASUREMENT_PATH / \"runtime_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23ee49-a5fe-4fd3-8c01-8ff76944d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime vs number of events in file\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(out[\"num_entries\"], out[\"runtime\"], marker=\"x\")\n",
    "ax.set_xlabel(\"number of events\")\n",
    "ax.set_ylabel(\"runtime [s]\")\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xvals = np.linspace(*xlim, 100)\n",
    "ax.plot(xvals, xvals/(5*1_000), label=\"5 kHz\", linestyle=\"-\", c=\"C1\")\n",
    "ax.plot(xvals, xvals/(10*1_000), label=\"10 kHz\", linestyle=\"--\", c=\"C2\")\n",
    "ax.plot(xvals, xvals/(20*1_000), label=\"20 kHz\", linestyle=\":\", c=\"C3\")\n",
    "ax.set_xlim([0, xlim[1]])\n",
    "ax.set_ylim([0, ylim[1]])\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig(MEASUREMENT_PATH / \"runtime_vs_nevts.pdf\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version,-language_info.version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
