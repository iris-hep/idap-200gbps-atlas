{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True\n",
    "DATASET = \"\" # \"mc_1TB\", \"multi_1TB\" - 4 dids, \"all\"\n",
    "IGNORE_CACHE = False\n",
    "DOWNLOAD_SX_RESULT = False\n",
    "\n",
    "DASK_CLIENT = \"local\" # local or scheduler\n",
    "DASK_SCHEDULER_ADDRESS = \"\"\n",
    "PROFILE = False\n",
    "DASK_REPORT = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import uproot\n",
    "from dask.distributed import Client, LocalCluster, performance_report\n",
    "from datasets import determine_dataset\n",
    "\n",
    "import servicex as sx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedFormatter(logging.Formatter):\n",
    "    \"\"\"Logging formatter that adds an elapsed time record since it was\n",
    "    first created. Error messages are printed relative to when the code\n",
    "    started - which makes it easier to understand how long operations took.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fmt=\"%(elapsed)s - %(levelname)s - %(name)s - %(message)s\"):\n",
    "        super().__init__(fmt)\n",
    "        self._start_time = time.time()\n",
    "\n",
    "    def format(self, record):\n",
    "        record.elapsed = f\"{time.time() - self._start_time:0>9.4f}\"\n",
    "        return super().format(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(input_filenames=None):\n",
    "    \n",
    "    ELECTRON_PT_THRESHOLD = 100e3\n",
    "    \n",
    "    import uproot\n",
    "    import awkward as ak\n",
    "    \n",
    "    BRANCH_LIST = [\n",
    "        \"AnalysisJetsAuxDyn.pt\", \"AnalysisJetsAuxDyn.eta\", \"AnalysisJetsAuxDyn.phi\", \"AnalysisJetsAuxDyn.m\",\n",
    "        \"AnalysisElectronsAuxDyn.pt\", \"AnalysisElectronsAuxDyn.eta\", \"AnalysisElectronsAuxDyn.phi\",\n",
    "        \"AnalysisElectronsAuxDyn.m\", \"AnalysisMuonsAuxDyn.pt\", \"AnalysisMuonsAuxDyn.eta\",\n",
    "        \"AnalysisMuonsAuxDyn.phi\", \"AnalysisJetsAuxDyn.EnergyPerSampling\", \"AnalysisJetsAuxDyn.SumPtTrkPt500\",\n",
    "        \"AnalysisJetsAuxDyn.TrackWidthPt1000\", \"PrimaryVerticesAuxDyn.z\", \"PrimaryVerticesAuxDyn.x\",\n",
    "        \"PrimaryVerticesAuxDyn.y\", \"AnalysisJetsAuxDyn.NumTrkPt500\", \"AnalysisJetsAuxDyn.NumTrkPt1000\",\n",
    "        \"AnalysisJetsAuxDyn.SumPtChargedPFOPt500\", \"AnalysisJetsAuxDyn.Timing\",\n",
    "        \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_eta\", \"AnalysisJetsAuxDyn.ActiveArea4vec_eta\",\n",
    "        \"AnalysisJetsAuxDyn.DetectorEta\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_phi\",\n",
    "        \"AnalysisJetsAuxDyn.ActiveArea4vec_phi\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_m\",\n",
    "        \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_pt\", \"AnalysisJetsAuxDyn.EMFrac\",\n",
    "        \"AnalysisJetsAuxDyn.Width\", \"AnalysisJetsAuxDyn.ActiveArea4vec_m\", \"AnalysisJetsAuxDyn.ActiveArea4vec_pt\",\n",
    "        \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksWidth\", \"AnalysisJetsAuxDyn.PSFrac\",\n",
    "        \"AnalysisJetsAuxDyn.JVFCorr\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksC1\",\n",
    "        \"AnalysisJetsAuxDyn.DFCommonJets_fJvt\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_NTracks\",\n",
    "        \"AnalysisJetsAuxDyn.GhostMuonSegmentCount\", \"AnalysisMuonsAuxDyn.muonSegmentLinks\",\n",
    "        \"AnalysisMuonsAuxDyn.msOnlyExtrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.extrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.inDetTrackParticleLink\", \"AnalysisMuonsAuxDyn.muonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.momentumBalanceSignificance\", \"AnalysisMuonsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "        \"AnalysisMuonsAuxDyn.scatteringCurvatureSignificance\", \"AnalysisMuonsAuxDyn.scatteringNeighbourSignificance\",\n",
    "        \"AnalysisMuonsAuxDyn.neflowisol20_CloseByCorr\", \"AnalysisMuonsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisMuonsAuxDyn.topoetcone30\", \"AnalysisMuonsAuxDyn.topoetcone40\", \"AnalysisMuonsAuxDyn.neflowisol20\",\n",
    "        \"AnalysisMuonsAuxDyn.segmentDeltaEta\", \"AnalysisMuonsAuxDyn.DFCommonJetDr\",\n",
    "        \"AnalysisMuonsAuxDyn.combinedTrackParticleLink\", \"AnalysisMuonsAuxDyn.InnerDetectorPt\",\n",
    "        \"AnalysisMuonsAuxDyn.MuonSpectrometerPt\", \"AnalysisMuonsAuxDyn.clusterLink\",\n",
    "        \"AnalysisMuonsAuxDyn.spectrometerFieldIntegral\", \"AnalysisElectronsAuxDyn.ambiguityLink\",\n",
    "        \"AnalysisMuonsAuxDyn.EnergyLoss\", \"AnalysisJetsAuxDyn.NNJvtPass\", \"AnalysisElectronsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "        \"AnalysisElectronsAuxDyn.topoetcone20ptCorrection\", \"AnalysisElectronsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500_CloseByCorr\",\n",
    "        \"AnalysisElectronsAuxDyn.DFCommonElectronsECIDSResult\", \"AnalysisElectronsAuxDyn.neflowisol20\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500\", \"AnalysisMuonsAuxDyn.ptcone40\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000_CloseByCorr\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000\", \"AnalysisMuonsAuxDyn.ptvarcone40\",\n",
    "        \"AnalysisElectronsAuxDyn.f1\", \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt500\",\n",
    "        \"PrimaryVerticesAuxDyn.vertexType\", \"AnalysisMuonsAuxDyn.ptvarcone30\", \"AnalysisMuonsAuxDyn.ptcone30\",\n",
    "        \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt1000\",\n",
    "        \"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt500\", \"AnalysisMuonsAuxDyn.CaloLRLikelihood\"\n",
    "    ]\n",
    "\n",
    "    filter_name = lambda x: x in BRANCH_LIST\n",
    "\n",
    "    with uproot.open({input_filenames:\"CollectionTree\"}, filter_name=filter_name) as f:\n",
    "        branches = {}\n",
    "        for b in BRANCH_LIST:\n",
    "            try:\n",
    "                branches[b] = f[b].array()\n",
    "                if \"m_persKey\" in str(branches[b].type):\n",
    "                    branches[b] = ak.count_nonzero(branches[b][\"m_persIndex\"], axis=-1)\n",
    "                elif \"var * var *\" in str(branches[b].type):\n",
    "                    branches[b] = ak.sum(branches[b], axis=1)                \n",
    "            except:\n",
    "                pass\n",
    "        ak_table = ak.Array(branches)\n",
    "        ak_table = ak_table[ak.min(ak_table[\"AnalysisElectronsAuxDyn.pt\"] > ELECTRON_PT_THRESHOLD, axis=1)]\n",
    "\n",
    "        return {\"servicex_reduction\": ak_table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_servicex(\n",
    "    ignore_cache: bool,\n",
    "    num_files: int,\n",
    "    ds_names: List[str],\n",
    "    download: bool,\n",
    "    query: Tuple[Any, str],\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"Load and execute the servicex query. Returns a complete list of paths\n",
    "    (be they local or url's) for the root or parquet files.\n",
    "    \"\"\"\n",
    "    logging.info(\"Building ServiceX query\")\n",
    "\n",
    "    # Do the query.\n",
    "    # TODO: Where is the enum that does DeliveryEnum come from?\n",
    "    # TODO: Why does `Sample` fail type checking - that type ignore has already hidden one bug!\n",
    "    # TODO: If I change Name after running, cache seems to fail (name doesn't track).\n",
    "    # TODO: If you change the name of the item you'll get a multiple cache hit!\n",
    "    # TODO: `servicex cache list` doesn't work and can't figure out how to make it work.\n",
    "    # TODO: servicex_query_cache.json is being ignored (feature?)\n",
    "    # TODO: Why does OutputFormat and delivery not work as enums? And fail typechecking with\n",
    "    #       strings?\n",
    "    # TODO: If some of these submissions work and others do not, we lose the ability to track the\n",
    "    #       ones we fired off.\n",
    "    #       an example is a title that is longer than 128 characters causes an immediate crash -\n",
    "    #       but other queries\n",
    "    #       already worked. Cache recovery @ the server would mean this wasn't important.\n",
    "\n",
    "    spec = sx.ServiceXSpec(\n",
    "        General=sx.General(\n",
    "            ServiceX=\"uc-af\",\n",
    "            Codegen=query[1],\n",
    "            OutputFormat=sx.ResultFormat.root,  # type: ignore\n",
    "            Delivery=(\"LocalCache\" if download else \"SignedURLs\"),  # type: ignore\n",
    "        ),\n",
    "        Sample=[\n",
    "            sx.Sample(\n",
    "                Name=f\"speed_test_{ds_name}\"[0:128],\n",
    "                RucioDID=ds_name,\n",
    "                Query=query[0],\n",
    "                NFiles=num_files,\n",
    "                IgnoreLocalCache=ignore_cache,\n",
    "            )  # type: ignore\n",
    "            for ds_name in ds_names\n",
    "        ],\n",
    "    )\n",
    "    for ds_name in ds_names:\n",
    "        logging.info(f\"Querying dataset {ds_name}\")\n",
    "    if num_files == 0:\n",
    "        logging.info(\"Running on the full dataset.\")\n",
    "    else:\n",
    "        logging.info(f\"Running on {num_files} files of dataset.\")\n",
    "\n",
    "    logging.info(\"Starting ServiceX query\")\n",
    "    results = sx.deliver(spec)\n",
    "    assert results is not None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_count(\n",
    "    ds_name: str, steps_per_file: int, files: List[str]\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"Calculate the non zero fields in the files.\n",
    "\n",
    "    Args:\n",
    "        steps_per_file (int): The number of steps to split the file into.\n",
    "        files (List[str]): The list of files in which to count the fields.\n",
    "\n",
    "    Returns:\n",
    "        _: DASK graph for the total count.\n",
    "    \"\"\"\n",
    "    data, report_to_be = uproot.dask(\n",
    "        {f: \"servicex_reduction\" for f in files},\n",
    "        open_files=False,\n",
    "        steps_per_file=steps_per_file,\n",
    "        allow_read_errors_with_report=True,\n",
    "    )\n",
    "\n",
    "    # Now, do the counting.\n",
    "    # The straight forward way to do this leads to a very large dask graph. We can\n",
    "    # do a little prep work here and make it more clean.\n",
    "    logging.debug(\n",
    "        f\"{ds_name}: Generating the dask compute graph for\"\n",
    "        f\" {len(data.fields)} fields\"  # type: ignore\n",
    "    )\n",
    "\n",
    "    total_count = 0\n",
    "    assert isinstance(data, dak.Array)  # type: ignore\n",
    "    for field in data.fields:\n",
    "        logging.debug(f\"{ds_name}: Counting field {field}\")\n",
    "        if str(data[field].type.content).startswith(\"var\"):\n",
    "            count = ak.count_nonzero(data[field], axis=-1)\n",
    "            for _ in range(count.ndim - 1):  # type: ignore\n",
    "                count = ak.count_nonzero(count, axis=-1)\n",
    "\n",
    "            total_count = total_count + count  # type: ignore\n",
    "        else:\n",
    "            # We get a not implemented error when we try to do this\n",
    "            # on leaves like run-number or event-number (e.g. scalars)\n",
    "            # Maybe we should just be adding a 1. :-)\n",
    "            logging.debug(\n",
    "                f\"{ds_name}: Field {field} is not a scalar field. Skipping count.\"\n",
    "            )\n",
    "\n",
    "    total_count = ak.count_nonzero(total_count, axis=0)\n",
    "\n",
    "    n_optimized_tasks = len(dask.optimize(total_count)[0].dask)  # type: ignore\n",
    "    logging.log(\n",
    "        logging.INFO,\n",
    "        f\"{ds_name}: Number of tasks in the dask graph: optimized: \"\n",
    "        f\"{n_optimized_tasks:,} \"  # type: ignore\n",
    "        f\"unoptimized: {len(total_count.dask):,}\",  # type: ignore\n",
    "    )\n",
    "\n",
    "    # total_count.visualize(optimize_graph=True)  # type: ignore\n",
    "    # opt = Path(\"mydask.png\")\n",
    "    # opt.replace(\"dask-optimized.png\")\n",
    "    # total_count.visualize(optimize_graph=False)  # type: ignore\n",
    "    # opt.replace(\"dask-unoptimized.png\")\n",
    "\n",
    "    return report_to_be, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    ignore_cache: bool = False,\n",
    "    num_files: int = 10,\n",
    "    dask_report: bool = False,\n",
    "    ds_names: Optional[List[str]] = None,\n",
    "    download_sx_result: bool = False,\n",
    "    steps_per_file: int = 3,\n",
    "    query: Optional[Tuple[Any, str]] = None,\n",
    "):\n",
    "    \"\"\"Match the operations found in `materialize_branches` notebook:\n",
    "    Load all the branches from some dataset, and then count the flattened\n",
    "    number of items, and, finally, print them out.\n",
    "    \"\"\"\n",
    "    assert query is not None, \"No query provided to run.\"\n",
    "\n",
    "    # Make sure there is a file here to save the SX query ID's to\n",
    "    # improve performance!\n",
    "    sx_query_ids = Path(\"./servicex_query_cache.json\")\n",
    "    if not sx_query_ids.exists():\n",
    "        sx_query_ids.touch()\n",
    "\n",
    "    assert ds_names is not None\n",
    "    dataset_files = query_servicex(\n",
    "        ignore_cache=ignore_cache,\n",
    "        num_files=num_files,\n",
    "        ds_names=ds_names,\n",
    "        download=download_sx_result,\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    for ds, files in dataset_files.items():\n",
    "        logging.info(f\"Dataset {ds} has {len(files)} files\")\n",
    "        assert len(files) > 0, \"No files found in the dataset\"\n",
    "\n",
    "    # now materialize everything.\n",
    "    logging.info(\n",
    "        f\"Using `uproot.dask` to open files (splitting files {steps_per_file} ways).\"\n",
    "    )\n",
    "    # The 20 steps per file was tuned for this query and 8 CPU's and 32 GB of memory.\n",
    "    logging.info(\"Starting build of DASK graphs\")\n",
    "    all_dask_data = {\n",
    "        k: calculate_total_count(k, steps_per_file, files)\n",
    "        for k, files in dataset_files.items()\n",
    "    }\n",
    "    logging.info(\"Done building DASK graphs.\")\n",
    "\n",
    "    # Do the calc now.\n",
    "    logging.info(\"Computing the total count\")\n",
    "    all_tasks = {k: v[1] for k, v in all_dask_data.items()}\n",
    "    if dask_report:\n",
    "        with performance_report(filename=\"dask-report.html\"):\n",
    "            results = dask.compute(*all_tasks.values())  # type: ignore\n",
    "            result_dict = dict(zip(all_tasks.keys(), results))\n",
    "    else:\n",
    "        results = dask.compute(*all_tasks.values())  # type: ignore\n",
    "        result_dict = dict(zip(all_tasks.keys(), results))\n",
    "\n",
    "    for k, r in result_dict.items():\n",
    "        logging.info(f\"{k}: result = {r:,}\")\n",
    "\n",
    "    # Scan through for any exceptions that happened during the dask processing.\n",
    "    all_report_tasks = {k: v[0] for k, v in all_dask_data.items()}\n",
    "    all_reports = dask.compute(*all_report_tasks.values())  # type: ignore\n",
    "    for k, report_list in zip(all_report_tasks.keys(), all_reports):\n",
    "        for process in report_list:\n",
    "            if process.exception is not None:\n",
    "                logging.error(\n",
    "                    f\"Exception in process '{process.message}' on file {process.args[0]} \"\n",
    "                    \"for ds {k}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(ElapsedFormatter())\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "# Set the logging level based on the verbosity flag.\n",
    "# make sure the time comes out so people can \"track\" what is going on.\n",
    "if VERBOSE:\n",
    "    root_logger.setLevel(level=logging.INFO)\n",
    "else:\n",
    "    root_logger.setLevel(level=logging.WARNING)\n",
    "root_logger.addHandler(handler)\n",
    "\n",
    "\n",
    "steps_per_file = 1\n",
    "if DASK_CLIENT == \"local\":\n",
    "    # Do not know how to do it otherwise.\n",
    "    n_workers = 8\n",
    "    logging.debug(\"Creating local Dask cluster for {n_workers} workers\")\n",
    "    cluster = LocalCluster(\n",
    "        n_workers=n_workers, processes=False, threads_per_worker=1\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    steps_per_file = 20\n",
    "elif DASK_CLIENT == \"scheduler\":\n",
    "    logging.debug(\"Connecting to Dask scheduler at {DASK_SCHEDULER_ADDRESS}\")\n",
    "    client = Client(DASK_SCHEDULER_ADDRESS)\n",
    "    steps_per_file = 2\n",
    "\n",
    "\n",
    "ds_names = determine_dataset(DATASET)\n",
    "\n",
    "# Build the query\n",
    "query = (run_query, \"python\")\n",
    "\n",
    "\n",
    "# Now run the main function\n",
    "if PROFILE is False:\n",
    "    main(\n",
    "        ignore_cache=IGNORE_CACHE,\n",
    "        dask_report=DASK_REPORT,\n",
    "        ds_names=ds_names,\n",
    "        download_sx_result=DOWNLOAD_SX_RESULT,\n",
    "        steps_per_file=steps_per_file,\n",
    "        query=query,\n",
    "    )\n",
    "else:\n",
    "    cProfile.run(\n",
    "        \"main(ignore_cache=IGNORE_CACHE, \"\n",
    "        \"dask_report=DASK_REPORT, ds_name = ds_name, \"\n",
    "        \"download_sx_result=DOWNLOAD_SX_RESULT, steps_per_file=steps_per_file\"\n",
    "        \"query=query)\",\n",
    "        \"sx_materialize_branches.pstats\",\n",
    "    )\n",
    "    logging.info(\"Profiling data saved to `sx_materialize_branches.pstats`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
