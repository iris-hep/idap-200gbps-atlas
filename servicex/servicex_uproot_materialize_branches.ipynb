{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "VERBOSE = True\n",
    "DATASET = \"multi_1TB\" # \"mc_1TB\", \"multi_1TB\" - 4 dids, \"data_50TB\", all\"\n",
    "NUM_FILES = \"10\" # \"0\" for all files\n",
    "IGNORE_CACHE = False\n",
    "DOWNLOAD_SX_RESULT = True\n",
    "\n",
    "DASK_CLIENT = \"local\" # \"local\" or \"scheduler\"\n",
    "DASK_SCHEDULER_ADDRESS = \"tcp://dask-kyungeonchoi-56c93432-0.af-jupyter:8786\"\n",
    "DASK_REPORT = False\n",
    "STEPS_PER_FILE = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple\n",
    "import awkward as ak\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "from dask.distributed import Client, LocalCluster, performance_report\n",
    "import servicex as sx\n",
    "import uproot\n",
    "\n",
    "from datasets import determine_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ServiceX query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "def run_query(input_filenames=None):\n",
    "    \n",
    "    PERCENT = 1\n",
    "    \n",
    "    import uproot\n",
    "    import awkward as ak\n",
    "    \n",
    "    BRANCH_LIST = [\n",
    "        \"AnalysisJetsAuxDyn.pt\", \"AnalysisJetsAuxDyn.eta\", \"AnalysisJetsAuxDyn.phi\", \"AnalysisJetsAuxDyn.m\",\n",
    "        \"AnalysisElectronsAuxDyn.pt\", \"AnalysisElectronsAuxDyn.eta\", \"AnalysisElectronsAuxDyn.phi\",\n",
    "        \"AnalysisElectronsAuxDyn.m\", \"AnalysisMuonsAuxDyn.pt\", \"AnalysisMuonsAuxDyn.eta\",\n",
    "        \"AnalysisMuonsAuxDyn.phi\", \"AnalysisJetsAuxDyn.EnergyPerSampling\", \"AnalysisJetsAuxDyn.SumPtTrkPt500\",\n",
    "        \"AnalysisJetsAuxDyn.TrackWidthPt1000\", \"PrimaryVerticesAuxDyn.z\", \"PrimaryVerticesAuxDyn.x\",\n",
    "        \"PrimaryVerticesAuxDyn.y\", \"AnalysisJetsAuxDyn.NumTrkPt500\", \"AnalysisJetsAuxDyn.NumTrkPt1000\",\n",
    "        \"AnalysisJetsAuxDyn.SumPtChargedPFOPt500\", \"AnalysisJetsAuxDyn.Timing\",\n",
    "        \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_eta\", \"AnalysisJetsAuxDyn.ActiveArea4vec_eta\",\n",
    "        \"AnalysisJetsAuxDyn.DetectorEta\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_phi\",\n",
    "        \"AnalysisJetsAuxDyn.ActiveArea4vec_phi\", \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_m\",\n",
    "        \"AnalysisJetsAuxDyn.JetConstitScaleMomentum_pt\", \"AnalysisJetsAuxDyn.EMFrac\",\n",
    "        \"AnalysisJetsAuxDyn.Width\", \"AnalysisJetsAuxDyn.ActiveArea4vec_m\", \"AnalysisJetsAuxDyn.ActiveArea4vec_pt\",\n",
    "        \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksWidth\", \"AnalysisJetsAuxDyn.PSFrac\",\n",
    "        \"AnalysisJetsAuxDyn.JVFCorr\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_TracksC1\",\n",
    "        \"AnalysisJetsAuxDyn.DFCommonJets_fJvt\", \"AnalysisJetsAuxDyn.DFCommonJets_QGTagger_NTracks\",\n",
    "        \"AnalysisJetsAuxDyn.GhostMuonSegmentCount\", \"AnalysisMuonsAuxDyn.muonSegmentLinks\",\n",
    "        \"AnalysisMuonsAuxDyn.msOnlyExtrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.extrapolatedMuonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.inDetTrackParticleLink\", \"AnalysisMuonsAuxDyn.muonSpectrometerTrackParticleLink\",\n",
    "        \"AnalysisMuonsAuxDyn.momentumBalanceSignificance\", \"AnalysisMuonsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "        \"AnalysisMuonsAuxDyn.scatteringCurvatureSignificance\", \"AnalysisMuonsAuxDyn.scatteringNeighbourSignificance\",\n",
    "        \"AnalysisMuonsAuxDyn.neflowisol20_CloseByCorr\", \"AnalysisMuonsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisMuonsAuxDyn.topoetcone30\", \"AnalysisMuonsAuxDyn.topoetcone40\", \"AnalysisMuonsAuxDyn.neflowisol20\",\n",
    "        \"AnalysisMuonsAuxDyn.segmentDeltaEta\", \"AnalysisMuonsAuxDyn.DFCommonJetDr\",\n",
    "        \"AnalysisMuonsAuxDyn.combinedTrackParticleLink\", \"AnalysisMuonsAuxDyn.InnerDetectorPt\",\n",
    "        \"AnalysisMuonsAuxDyn.MuonSpectrometerPt\", \"AnalysisMuonsAuxDyn.clusterLink\",\n",
    "        \"AnalysisMuonsAuxDyn.spectrometerFieldIntegral\", \"AnalysisElectronsAuxDyn.ambiguityLink\",\n",
    "        \"AnalysisMuonsAuxDyn.EnergyLoss\", \"AnalysisJetsAuxDyn.NNJvtPass\", \"AnalysisElectronsAuxDyn.topoetcone20_CloseByCorr\",\n",
    "        \"AnalysisElectronsAuxDyn.topoetcone20ptCorrection\", \"AnalysisElectronsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500_CloseByCorr\",\n",
    "        \"AnalysisElectronsAuxDyn.DFCommonElectronsECIDSResult\", \"AnalysisElectronsAuxDyn.neflowisol20\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt500\", \"AnalysisMuonsAuxDyn.ptcone40\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000_CloseByCorr\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000\", \"AnalysisMuonsAuxDyn.ptvarcone40\",\n",
    "        \"AnalysisElectronsAuxDyn.f1\", \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt500\",\n",
    "        \"PrimaryVerticesAuxDyn.vertexType\", \"AnalysisMuonsAuxDyn.ptvarcone30\", \"AnalysisMuonsAuxDyn.ptcone30\",\n",
    "        \"AnalysisMuonsAuxDyn.ptcone20_Nonprompt_All_MaxWeightTTVA_pt1000\",\n",
    "        \"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt500\", \"AnalysisMuonsAuxDyn.CaloLRLikelihood\"\n",
    "    ]\n",
    "\n",
    "    filter_name = lambda x: x in BRANCH_LIST\n",
    "\n",
    "    with uproot.open({input_filenames:\"CollectionTree\"}, filter_name=filter_name) as f:\n",
    "    # with uproot.open({input_filenames:\"CollectionTree\"}) as f:\n",
    "        branches = {}\n",
    "        for b in BRANCH_LIST:\n",
    "            try:\n",
    "                branches[b] = f[b].array()\n",
    "                if \"Link\" in str(branches[b].type):\n",
    "                    branches[b] = ak.Array(range(len(branches[b])))\n",
    "                elif \"var * var *\" in str(branches[b].type):\n",
    "                    branches[b] = ak.sum(branches[b], axis=1)                \n",
    "            except:\n",
    "                pass\n",
    "        ak_table = ak.Array(branches)\n",
    "\n",
    "        end = int(len(ak_table)*PERCENT/100)\n",
    "        ak_table = ak_table[:end]\n",
    "\n",
    "        return {\"servicex_reduction\": ak_table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "def build_servicex_spec(\n",
    "    ignore_cache: bool,\n",
    "    num_files: int,\n",
    "    download: bool\n",
    "):\n",
    "    \"\"\"Load the servicex query \n",
    "    \"\"\"\n",
    "\n",
    "    # List of RucioDIDs    \n",
    "    ds_names = determine_dataset(DATASET)\n",
    "\n",
    "    # Build the query\n",
    "    query = (run_query, \"python\")\n",
    "\n",
    "    for ds_name in ds_names:\n",
    "        print(f\"Querying dataset {ds_name}\")\n",
    "    if num_files == 0:\n",
    "        print(\"Running on the full dataset.\")\n",
    "    else:\n",
    "        print(f\"Running on {num_files} files of dataset.\")\n",
    "\n",
    "    spec = sx.ServiceXSpec(\n",
    "        General=sx.General(\n",
    "            ServiceX=\"testing4\",\n",
    "            Codegen=query[1],\n",
    "            OutputFormat=sx.ResultFormat.root,  # type: ignore\n",
    "            Delivery=(\"LocalCache\" if download else \"SignedURLs\"),  # type: ignore\n",
    "        ),\n",
    "        Sample=[\n",
    "            sx.Sample(\n",
    "                Name=f\"{ds_name}\"[0:40],\n",
    "                RucioDID=ds_name,\n",
    "                Function=query[0],\n",
    "                NFiles=num_files,\n",
    "                IgnoreLocalCache=ignore_cache,\n",
    "            )  # type: ignore\n",
    "            for ds_name in ds_names\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "sx_spec = build_servicex_spec(IGNORE_CACHE, NUM_FILES, DOWNLOAD_SX_RESULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ServiceX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Returns a complete list of paths (be they local or url's) for the root or parquet files.\n",
    "results = sx.deliver(sx_spec)\n",
    "\n",
    "# Print what we got\n",
    "for sample in results:\n",
    "    print(f\"Dataset {sample} contains {len(results[sample])} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "if DASK_CLIENT == \"local\":\n",
    "    # Do not know how to do it otherwise.\n",
    "    n_workers = 8\n",
    "    print(\"Creating local Dask cluster for {n_workers} workers\")\n",
    "    cluster = LocalCluster(\n",
    "        n_workers=n_workers, processes=False, threads_per_worker=1\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    steps_per_file = 20\n",
    "elif DASK_CLIENT == \"scheduler\":\n",
    "    print(\"Connecting to Dask scheduler at {DASK_SCHEDULER_ADDRESS}\")\n",
    "    client = Client(DASK_SCHEDULER_ADDRESS)\n",
    "    steps_per_file = 2\n",
    "else:\n",
    "    print(\"Unknown dask client!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "def calculate_total_count(\n",
    "    ds_name: str, steps_per_file: int, files: List[str]\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"Calculate the non zero fields in the files.\n",
    "\n",
    "    Args:\n",
    "        steps_per_file (int): The number of steps to split the file into.\n",
    "        files (List[str]): The list of files in which to count the fields.\n",
    "\n",
    "    Returns:\n",
    "        _: DASK graph for the total count.\n",
    "    \"\"\"\n",
    "    data, report_to_be = uproot.dask(\n",
    "        {f: \"servicex_reduction\" for f in files},\n",
    "        open_files=False,\n",
    "        steps_per_file=steps_per_file,\n",
    "        allow_read_errors_with_report=True,\n",
    "    )\n",
    "\n",
    "    # Now, do the counting.\n",
    "    # The straight forward way to do this leads to a very large dask graph. We can\n",
    "    # do a little prep work here and make it more clean.\n",
    "    print(\n",
    "        f\"{ds_name}: Generating the dask compute graph for\"\n",
    "        f\" {len(data.fields)} fields\"  # type: ignore\n",
    "    )\n",
    "\n",
    "    total_count = 0\n",
    "    assert isinstance(data, dak.Array)  # type: ignore\n",
    "    for field in data.fields:\n",
    "        # print(f\"{ds_name}: Counting field {field}\")\n",
    "        _counter_to_add = ak.count_nonzero(data[field], axis=-1)  # reduce innermost\n",
    "\n",
    "    total_count = total_count + _counter_to_add  # sum 1-dim array built from new branch\n",
    "\n",
    "    total_count = ak.count_nonzero(total_count, axis=0)  # reduce to int\n",
    "        \n",
    "    n_optimized_tasks = len(dask.optimize(total_count)[0].dask)  # type: ignore\n",
    "    print(\n",
    "        f\"{ds_name}: Number of tasks in the dask graph: optimized: \"\n",
    "        f\"{n_optimized_tasks:,} \"  # type: ignore\n",
    "        f\"unoptimized: {len(total_count.dask):,}\",  # type: ignore\n",
    "    )\n",
    "\n",
    "    # total_count.visualize(optimize_graph=True)  # type: ignore\n",
    "    # opt = Path(\"mydask.png\")\n",
    "    # opt.replace(\"dask-optimized.png\")\n",
    "    # total_count.visualize(optimize_graph=False)  # type: ignore\n",
    "    # opt.replace(\"dask-unoptimized.png\")\n",
    "\n",
    "    return report_to_be, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# now materialize everything.\n",
    "print(\n",
    "    f\"Using `uproot.dask` to open files (splitting files {STEPS_PER_FILE} ways).\"\n",
    ")\n",
    "# The 20 steps per file was tuned for this query and 8 CPU's and 32 GB of memory.\n",
    "print(\"Starting build of DASK graphs\")\n",
    "all_dask_data = {\n",
    "    k: calculate_total_count(k, STEPS_PER_FILE, files)\n",
    "    for k, files in results.items()\n",
    "}\n",
    "print(\"Done building DASK graphs.\")\n",
    "\n",
    "# Do the calc now.\n",
    "print(\"Computing the total count\")\n",
    "all_tasks = {k: v[1] for k, v in all_dask_data.items()}\n",
    "if DASK_REPORT:\n",
    "    with performance_report(filename=\"dask-report.html\"):\n",
    "        results = dask.compute(*all_tasks.values())  # type: ignore\n",
    "        result_dict = dict(zip(all_tasks.keys(), results))\n",
    "else:\n",
    "    results = dask.compute(*all_tasks.values())  # type: ignore\n",
    "    result_dict = dict(zip(all_tasks.keys(), results))\n",
    "\n",
    "for k, r in result_dict.items():\n",
    "    print(f\"{k}: result = {r:,}\")\n",
    "\n",
    "# Scan through for any exceptions that happened during the dask processing.\n",
    "all_report_tasks = {k: v[0] for k, v in all_dask_data.items()}\n",
    "all_reports = dask.compute(*all_report_tasks.values())  # type: ignore\n",
    "for k, report_list in zip(all_report_tasks.keys(), all_reports):\n",
    "    for process in report_list:\n",
    "        if process.exception is not None:\n",
    "            print(\n",
    "                f\"Exception in process '{process.message}' on file {process.args[0]} \"\n",
    "                \"for ds {k}\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
